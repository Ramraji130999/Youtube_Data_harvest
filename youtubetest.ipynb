{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-python-client\n",
      "  Using cached google_api_python_client-2.119.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting httplib2<1.dev0,>=0.15.0 (from google-api-python-client)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth<3.0.0.dev0,>=1.19.0 (from google-api-python-client)\n",
      "  Using cached google_auth-2.28.1-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-httplib2>=0.1.0 (from google-api-python-client)\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client)\n",
      "  Using cached google_api_core-2.17.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Using cached googleapis_common_protos-1.62.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting requests<3.0.0.dev0,>=2.18.0 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.dev0,>=0.15.0->google-api-python-client)\n",
      "  Using cached pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client)\n",
      "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached google_api_python_client-2.119.0-py2.py3-none-any.whl (12.2 MB)\n",
      "Using cached google_api_core-2.17.1-py3-none-any.whl (137 kB)\n",
      "Using cached google_auth-2.28.1-py2.py3-none-any.whl (186 kB)\n",
      "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Using cached googleapis_common_protos-1.62.0-py2.py3-none-any.whl (228 kB)\n",
      "Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Using cached pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl (100 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: urllib3, uritemplate, pyparsing, pyasn1, protobuf, idna, charset-normalizer, certifi, cachetools, rsa, requests, pyasn1-modules, httplib2, googleapis-common-protos, google-auth, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "Successfully installed cachetools-5.3.3 certifi-2024.2.2 charset-normalizer-3.3.2 google-api-core-2.17.1 google-api-python-client-2.119.0 google-auth-2.28.1 google-auth-httplib2-0.2.0 googleapis-common-protos-1.62.0 httplib2-0.22.0 idna-3.6 protobuf-4.25.3 pyasn1-0.5.1 pyasn1-modules-0.3.0 pyparsing-3.1.1 requests-2.31.0 rsa-4.9 uritemplate-4.1.1 urllib3-2.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo\n",
      "  Using cached pymongo-4.6.2-cp312-cp312-win_amd64.whl.metadata (22 kB)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
      "  Using cached dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Using cached pymongo-4.6.2-cp312-cp312-win_amd64.whl (472 kB)\n",
      "Using cached dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "Installing collected packages: dnspython, pymongo\n",
      "Successfully installed dnspython-2.6.1 pymongo-4.6.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Using cached psycopg2-2.9.9-cp312-cp312-win_amd64.whl.metadata (4.5 kB)\n",
      "Using cached psycopg2-2.9.9-cp312-cp312-win_amd64.whl (1.2 MB)\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.1-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy<2,>=1.26.0 (from pandas)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nandh\\desktop\\new folder (2)\\.venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nandh\\desktop\\new folder (2)\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.2.1-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-1.26.4 pandas-2.2.1 pytz-2024.1 tzdata-2024.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Using cached streamlit-1.31.1-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Using cached altair-5.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Using cached blinker-1.7.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\nandh\\desktop\\new folder (2)\\.venv\\lib\\site-packages (from streamlit) (5.3.3)\n",
      "Collecting click<9,>=7.0 (from streamlit)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting importlib-metadata<8,>=1.4 (from streamlit)\n",
      "  Using cached importlib_metadata-7.0.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in c:\\users\\nandh\\desktop\\new folder (2)\\.venv\\lib\\site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<24,>=16.8 in c:\\users\\nandh\\desktop\\new folder (2)\\.venv\\lib\\site-packages (from streamlit) (23.2)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in c:\\users\\nandh\\desktop\\new folder (2)\\.venv\\lib\\site-packages (from streamlit) (2.2.1)\n",
      "Collecting pillow<11,>=7.1.0 (from streamlit)\n",
      "  Using cached pillow-10.2.0-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in c:\\users\\nandh\\desktop\\new folder (2)\\.venv\\lib\\site-packages (from streamlit) (4.25.3)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Using cached pyarrow-15.0.0-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.3 in c:\\users\\nandh\\desktop\\new folder (2)\\.venv\\lib\\site-packages (from streamlit) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\nandh\\desktop\\new folder (2)\\.venv\\lib\\site-packages (from streamlit) (2.31.0)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit)\n",
      "  Using cached rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tenacity<9,>=8.1.0 (from streamlit)\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting typing-extensions<5,>=4.3.0 (from streamlit)\n",
      "  Using cached typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting tzlocal<6,>=1.1 (from streamlit)\n",
      "  Using cached tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting validators<1,>=0.2 (from streamlit)\n",
      "  Using cached validators-0.22.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Using cached GitPython-3.1.42-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.8.1b0-py2.py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\nandh\\desktop\\new folder (2)\\.venv\\lib\\site-packages (from streamlit) (6.4)\n",
      "Collecting watchdog>=2.1.5 (from streamlit)\n",
      "  Using cached watchdog-4.0.0-py3-none-win_amd64.whl.metadata (37 kB)\n",
      "Collecting jinja2 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema-4.21.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting toolz (from altair<6,>=4.0->streamlit)\n",
      "  Using cached toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\nandh\\desktop\\new folder (2)\\.venv\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<8,>=1.4->streamlit)\n",
      "  Using cached zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nandh\\desktop\\new folder (2)\\.venv\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nandh\\desktop\\new folder (2)\\.venv\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nandh\\desktop\\new folder (2)\\.venv\\lib\\site-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nandh\\desktop\\new folder (2)\\.venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nandh\\desktop\\new folder (2)\\.venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nandh\\desktop\\new folder (2)\\.venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nandh\\desktop\\new folder (2)\\.venv\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2024.2.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\nandh\\desktop\\new folder (2)\\.venv\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.17.2)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->altair<6,>=4.0->streamlit)\n",
      "  Using cached MarkupSafe-2.1.5-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached referencing-0.33.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached rpds_py-0.18.0-cp312-none-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached streamlit-1.31.1-py2.py3-none-any.whl (8.4 MB)\n",
      "Using cached altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "Using cached blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
      "Using cached importlib_metadata-7.0.1-py3-none-any.whl (23 kB)\n",
      "Using cached pillow-10.2.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "Using cached pyarrow-15.0.0-cp312-cp312-win_amd64.whl (25.3 MB)\n",
      "Using cached pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "Using cached rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Using cached tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Using cached validators-0.22.0-py3-none-any.whl (26 kB)\n",
      "Using cached watchdog-4.0.0-py3-none-win_amd64.whl (82 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Using cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Using cached jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Using cached toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp312-cp312-win_amd64.whl (17 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached referencing-0.33.0-py3-none-any.whl (26 kB)\n",
      "Using cached rpds_py-0.18.0-cp312-none-win_amd64.whl (206 kB)\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: zipp, watchdog, validators, tzlocal, typing-extensions, toolz, toml, tenacity, smmap, rpds-py, pyarrow, pillow, mdurl, MarkupSafe, click, blinker, attrs, referencing, markdown-it-py, jinja2, importlib-metadata, gitdb, rich, pydeck, jsonschema-specifications, gitpython, jsonschema, altair, streamlit\n",
      "Successfully installed MarkupSafe-2.1.5 altair-5.2.0 attrs-23.2.0 blinker-1.7.0 click-8.1.7 gitdb-4.0.11 gitpython-3.1.42 importlib-metadata-7.0.1 jinja2-3.1.3 jsonschema-4.21.1 jsonschema-specifications-2023.12.1 markdown-it-py-3.0.0 mdurl-0.1.2 pillow-10.2.0 pyarrow-15.0.0 pydeck-0.8.1b0 referencing-0.33.0 rich-13.7.0 rpds-py-0.18.0 smmap-5.0.1 streamlit-1.31.1 tenacity-8.2.3 toml-0.10.2 toolz-0.12.1 typing-extensions-4.10.0 tzlocal-5.2 validators-0.22.0 watchdog-4.0.0 zipp-3.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pymongo\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#api key connection\n",
    "def Api_connect():\n",
    "    Api_Id=\"AIzaSyDRHSj0Gft1ojCYAuM7Fw9Q5n6wrVEUeIo\"\n",
    "\n",
    "    api_service_name=\"youtube\"\n",
    "    api_version=\"v3\"\n",
    "\n",
    "    youtube=build(api_service_name,api_version,developerKey=Api_Id)\n",
    "\n",
    "    return youtube\n",
    "\n",
    "youtube=Api_connect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets channel information\n",
    "def get_channel_info(channel_id):\n",
    "    request=youtube.channels().list(\n",
    "        part=\"snippet,ContentDetails,statistics\",\n",
    "        id=channel_id\n",
    "    )\n",
    "    response=request.execute()\n",
    "\n",
    "    for i in response ['items']:  \n",
    "        data=dict(Channel_Name=i[\"snippet\"][\"title\"],\n",
    "                  Channel_Id=i[\"id\"],\n",
    "                  Subscribers=i[\"statistics\"][\"subscriberCount\"],\n",
    "                  Views=i[\"statistics\"][\"viewCount\"],\n",
    "                  Total_Videos=i[\"statistics\"][\"videoCount\"],\n",
    "                  Channel_Description=i[\"snippet\"][\"description\"],\n",
    "                  Playlist_Id=i[\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"])\n",
    "    return data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get video ids\n",
    "def gets_videos_ids(channel_id):\n",
    "    video_ids=[]\n",
    "    response=youtube.channels().list(id=channel_id,\n",
    "                                     part='contentDetails').execute()\n",
    "    Playlist_Id=response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "\n",
    "    next_page_token=None\n",
    "\n",
    "    while True:\n",
    "        response1=youtube.playlistItems().list(\n",
    "                                            part='snippet',\n",
    "                                            playlistId=Playlist_Id,\n",
    "                                            maxResults=50,\n",
    "                                            pageToken=next_page_token).execute()\n",
    "\n",
    "        for i in range(len(response1['items'])):\n",
    "            video_ids.append(response1['items'][i]['snippet']['resourceId']['videoId'])\n",
    "        next_page_token=response1.get('nextpageToken')\n",
    "\n",
    "        if next_page_token is None:\n",
    "            break\n",
    "    return video_ids    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get video information\n",
    "def get_video_info(video_ids):\n",
    "    video_data=[]\n",
    "    for video_id in video_ids:\n",
    "        request=youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics\",\n",
    "            id=video_id\n",
    "        )\n",
    "        response=request.execute()\n",
    "\n",
    "        for item in response[\"items\"]:\n",
    "            data=dict(Channel_Name=item['snippet']['channelTitle'],\n",
    "                      Channel_Id=item['snippet']['channelId'],\n",
    "                      Video_Id=item['id'],\n",
    "                      Title=item['snippet']['title'],\n",
    "                      Tags=item['snippet'].get('tags'),\n",
    "                      Thumbnail=item['snippet']['thumbnails']['default']['url'],\n",
    "                      Description=item['snippet'].get('description'),\n",
    "                      Published_Date=item['snippet']['publishedAt'],\n",
    "                      Duration=item['contentDetails']['duration'],\n",
    "                      Views=item['statistics'].get('viewCount'),\n",
    "                      Likes=item['statistics'].get('likeCount'),\n",
    "                      Comments=item['statistics'].get('commentCount'),\n",
    "                      Favorite_Count=item['statistics']['favoriteCount'],\n",
    "                      Definition=item['contentDetails']['definition'],\n",
    "                      Caption_Status=item['contentDetails']['caption']\n",
    "                      )\n",
    "            video_data.append(data)\n",
    "    return video_data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get comment information\n",
    "def get_comment_info(video_ids):\n",
    "    Comment_data=[]\n",
    "    try:\n",
    "        for video_id in video_ids:\n",
    "            request=youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id, \n",
    "                maxResults=50\n",
    "            )\n",
    "            response=request.execute()\n",
    "\n",
    "            for item in response['items']:\n",
    "                data=dict(Comment_Id=item['snippet']['topLevelComment']['id'],\n",
    "                        Video_Id=item['snippet']['topLevelComment']['snippet']['videoId'],\n",
    "                        Comment_Text=item['snippet']['topLevelComment']['snippet']['textDisplay'],\n",
    "                        Comment_Author=item['snippet']['topLevelComment']['snippet']['authorDisplayName'],\n",
    "                        Comment_Published=item['snippet']['topLevelComment']['snippet']['publishedAt'])\n",
    "                \n",
    "                Comment_data.append(data)\n",
    "                \n",
    "    except:\n",
    "        pass\n",
    "    return Comment_data            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload to mongodb\n",
    "client=pymongo.MongoClient(\"mongodb+srv://ramanathan:Ramraji09@cluster0.q4lre4m.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\")\n",
    "db=client[\"Youtube_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get channel details:\n",
    "def channel_details(channel_id):\n",
    "    ch_details=get_channel_info(channel_id)\n",
    "    vi_ids=gets_videos_ids(channel_id)\n",
    "    vi_details=get_video_info(vi_ids)\n",
    "    com_details=get_comment_info(vi_ids)\n",
    "     \n",
    "    coll1=db[\"channel_details\"]\n",
    "    coll1.insert_one({\"channel_information\":ch_details,\"video_information\":vi_details,\"comment_information\":com_details})\n",
    "\n",
    "    return \"data entered successfully\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get channel details:\n",
    "def channel_details(channel_id):\n",
    "    ch_details=get_channel_info(channel_id)\n",
    "    vi_ids=gets_videos_ids(channel_id)\n",
    "    vi_details=get_video_info(vi_ids)\n",
    "    com_details=get_comment_info(vi_ids)\n",
    "     \n",
    "    coll1=db[\"channel_details\"]\n",
    "    coll1.insert_one({\"channel_information\":ch_details,\"video_information\":vi_details,\"comment_information\":com_details})\n",
    "\n",
    "    return \"data entered successfully\"\n",
    "\n",
    "#channels table creation\n",
    "def channels_table():\n",
    "    mydb = psycopg2.connect(host=\"localhost\",\n",
    "                            user=\"postgres\",\n",
    "                            password=\"Ramraji\",\n",
    "                            database=\"youtube_data\",\n",
    "                            port=\"5432\")\n",
    "\n",
    "    cursor = mydb.cursor()\n",
    "\n",
    "    drop_query='''drop table if exists channels'''\n",
    "    cursor.execute(drop_query)\n",
    "    mydb.commit()\n",
    "\n",
    "\n",
    "    try:\n",
    "        create_query='''create table if not exists channels(Channel_Name varchar(100),\n",
    "                                                                Channel_Id varchar(80) primary key,\n",
    "                                                                Subscribers bigint,\n",
    "                                                                Views bigint,\n",
    "                                                                Total_Videos int,\n",
    "                                                                Channel_Description text)'''\n",
    "        cursor.execute(create_query)\n",
    "        mydb.commit()\n",
    "            \n",
    "    except:\n",
    "        print(\"Channels table created already\")\n",
    "        \n",
    "    ch_list=[]\n",
    "    db=client[\"Youtube_data\"]\n",
    "    coll1=db[\"channel_details\"]\n",
    "    for ch_data in coll1.find({},{\"_id\":0,\"channel_information\":1}):\n",
    "            ch_list.append(ch_data[\"channel_information\"])\n",
    "    df=pd.DataFrame(ch_list)\n",
    "        \n",
    "    for index,row in df.iterrows():\n",
    "            insert_query='''insert into channels(Channel_Name,\n",
    "                                                Channel_Id,\n",
    "                                                Subscribers,\n",
    "                                                Views,\n",
    "                                                Total_Videos,\n",
    "                                                Channel_Description)\n",
    "            \n",
    "                                                values(%s,%s,%s,%s,%s,%s)'''\n",
    "            \n",
    "            values=(row['Channel_Name'],\n",
    "                    row['Channel_Id'],\n",
    "                    row['Subscribers'],\n",
    "                    row['Views'],\n",
    "                    row['Total_Videos'],\n",
    "                    row['Channel_Description'])\n",
    "            \n",
    "            try:\n",
    "                cursor.execute(insert_query,values)\n",
    "                mydb.commit()\n",
    "\n",
    "            except:\n",
    "                print(\"Channels values are inserted\") \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#videos table creation\n",
    "def videos_table():\n",
    "        mydb = psycopg2.connect(host=\"localhost\",\n",
    "                                user=\"postgres\",\n",
    "                                password=\"Ramraji\",\n",
    "                                database=\"youtube_data\",\n",
    "                                port=\"5432\")\n",
    "\n",
    "        cursor = mydb.cursor()\n",
    "\n",
    "        drop_query='''drop table if exists videos'''\n",
    "        cursor.execute(drop_query)\n",
    "        mydb.commit()\n",
    "\n",
    "        try:\n",
    "                create_query='''create table if not exists videos(Channel_Name varchar(100),\n",
    "                                                                Channel_Id varchar(100),\n",
    "                                                                Video_Id varchar(30) primary key,\n",
    "                                                                Title varchar(150),\n",
    "                                                                Tags text,\n",
    "                                                                Thumbnail varchar(200),\n",
    "                                                                Description text,\n",
    "                                                                Published_Date timestamp,\n",
    "                                                                Duration interval,\n",
    "                                                                Views bigint,\n",
    "                                                                Likes bigint,\n",
    "                                                                Comments int,\n",
    "                                                                Favorite_Count int,\n",
    "                                                                Definition varchar(10),\n",
    "                                                                Caption_Status varchar(50))'''\n",
    "                cursor.execute(create_query)\n",
    "                mydb.commit()\n",
    "                \n",
    "        except:\n",
    "                print(\"Channels table created already\")\n",
    "\n",
    "        vi_list=[]\n",
    "        db=client[\"Youtube_data\"]\n",
    "        coll1=db[\"channel_details\"]\n",
    "        for vi_data in coll1.find({},{\"_id\":0,\"video_information\":1}):\n",
    "                for i in range(len(vi_data[\"video_information\"])):\n",
    "                 vi_list.append(vi_data[\"video_information\"][i])\n",
    "        df1=pd.DataFrame(vi_list)\n",
    "                \n",
    "        for index,row in df1.iterrows():\n",
    "                insert_query='''insert into videos(Channel_Name,\n",
    "                                                        Channel_Id,\n",
    "                                                        Video_Id,\n",
    "                                                        Title,\n",
    "                                                        Tags,\n",
    "                                                        Thumbnail,\n",
    "                                                        Description,\n",
    "                                                        Published_Date,\n",
    "                                                        Duration ,\n",
    "                                                        Views,\n",
    "                                                        Likes ,\n",
    "                                                        Comments,\n",
    "                                                        Favorite_Count,\n",
    "                                                        Definition ,\n",
    "                                                        Caption_Status)\n",
    "                \n",
    "                                                        values(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)'''\n",
    "                \n",
    "                values=(row['Channel_Name'],\n",
    "                        row['Channel_Id'],\n",
    "                        row['Video_Id'],\n",
    "                        row['Title'],\n",
    "                        row['Tags'],\n",
    "                        row['Thumbnail'],\n",
    "                        row['Description'],\n",
    "                        row['Published_Date'],\n",
    "                        row['Duration'],\n",
    "                        row['Views'],\n",
    "                        row['Likes'],\n",
    "                        row['Comments'],\n",
    "                        row['Favorite_Count'],\n",
    "                        row['Definition'],\n",
    "                        row['Caption_Status']\n",
    "                        )\n",
    "                \n",
    "                try:\n",
    "                        cursor.execute(insert_query,values)\n",
    "                        mydb.commit()\n",
    "\n",
    "                except:\n",
    "                        print(\"Channels values are inserted\")             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comments table creation\n",
    "def comments_table():\n",
    "        mydb = psycopg2.connect(host=\"localhost\",\n",
    "                                user=\"postgres\",\n",
    "                                password=\"Ramraji\",\n",
    "                                database=\"youtube_data\",\n",
    "                                port=\"5432\")\n",
    "\n",
    "        cursor = mydb.cursor()\n",
    "\n",
    "        drop_query='''drop table if exists comments'''\n",
    "        cursor.execute(drop_query)\n",
    "        mydb.commit()\n",
    "\n",
    "\n",
    "        try:\n",
    "                create_query='''create table if not exists comments(Comment_Id varchar(100) primary key,\n",
    "                                                                Video_Id varchar(50),\n",
    "                                                                Comment_Text text,\n",
    "                                                                Comment_Author varchar(150),\n",
    "                                                                Comment_Published timestamp)'''\n",
    "                cursor.execute(create_query)\n",
    "                mydb.commit()\n",
    "                \n",
    "        except:\n",
    "                print(\"Channels table created already\")\n",
    "                \n",
    "        com_list=[]\n",
    "        db=client[\"Youtube_data\"]\n",
    "        coll1=db[\"channel_details\"]\n",
    "        for com_data in coll1.find({},{\"_id\":0,\"comment_information\":1}):\n",
    "                for i in range(len(com_data[\"comment_information\"])):\n",
    "                 com_list.append(com_data[\"comment_information\"][i])\n",
    "        df2=pd.DataFrame(com_list)     \n",
    "\n",
    "        for index,row in df2.iterrows():\n",
    "                insert_query='''insert into comments(Comment_Id,\n",
    "                                                        Video_Id,\n",
    "                                                        Comment_Text,\n",
    "                                                        Comment_Author,\n",
    "                                                        Comment_Published\n",
    "                                                        )\n",
    "                \n",
    "                                                        values(%s,%s,%s,%s,%s)'''\n",
    "                \n",
    "                values=(row['Comment_Id'],\n",
    "                        row['Video_Id'],\n",
    "                        row['Comment_Text'],\n",
    "                        row['Comment_Author'],\n",
    "                        row['Comment_Published']\n",
    "                        )\n",
    "                \n",
    "                try:\n",
    "                        cursor.execute(insert_query,values)\n",
    "                        mydb.commit()\n",
    "\n",
    "                except:\n",
    "                        print(\"Channels values are inserted\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tables():\n",
    "    channels_table()\n",
    "    videos_table()\n",
    "    comments_table()\n",
    "\n",
    "    return \"Table created successfully\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_channels_table():\n",
    "    ch_list=[]\n",
    "    db=client[\"Youtube_data\"]\n",
    "    coll1=db[\"channel_details\"]\n",
    "    for ch_data in coll1.find({},{\"_id\":0,\"channel_information\":1}):\n",
    "            ch_list.append(ch_data[\"channel_information\"])\n",
    "    df=st.dataframe(ch_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_videos_table():\n",
    "    vi_list=[]\n",
    "    db=client[\"Youtube_data\"]\n",
    "    coll1=db[\"channel_details\"]\n",
    "    for vi_data in coll1.find({},{\"_id\":0,\"video_information\":1}):\n",
    "        for i in range(len(vi_data[\"video_information\"])):\n",
    "                 vi_list.append(vi_data[\"video_information\"][i])\n",
    "    df1=st.dataframe(vi_list)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_comments_table():\n",
    "    com_list=[]\n",
    "    db=client[\"Youtube_data\"]\n",
    "    coll1=db[\"channel_details\"]\n",
    "    for com_data in coll1.find({},{\"_id\":0,\"comment_information\":1}):\n",
    "            for i in range(len(com_data[\"comment_information\"])):\n",
    "                 com_list.append(com_data[\"comment_information\"][i])\n",
    "    df2=st.dataframe(com_list)\n",
    "    return df2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 18:24:40.362 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\nandh\\Desktop\\New folder (2)\\.venv\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "#streamlit part\n",
    "\n",
    "with st.sidebar:\n",
    "    st.title(\":red[YOUTUBE DATA HARVESTING AND WAREHOUSING]\")\n",
    "    st.header(\"Skill Take Away\")\n",
    "    st.caption(\"Python Scripting\")\n",
    "    st.caption(\"Data Collection\")\n",
    "    st.caption(\"MongoDB\")\n",
    "    st.caption(\"API Integration\")\n",
    "    st.caption(\"Data Management using MongoDB and SQL\")\n",
    "\n",
    "channel_id=st.text_input(\"Enter the Channel ID\")\n",
    "\n",
    "if st.button(\"collect and store data\"):\n",
    "    ch_ids=[]\n",
    "    db=client[\"Youtube_data\"]\n",
    "    coll1=db[\"channel_details\"]\n",
    "    for ch_data in coll1.find({},{\"_id\":0,\"channel_information\":1}):\n",
    "        ch_ids.append(ch_data[\"channel_information\"][\"Channel_Id\"])\n",
    "\n",
    "    if channel_id in ch_ids:\n",
    "         st.success(\"Channel Details of the given Channel Id already exists\")\n",
    "\n",
    "    else:\n",
    "         insert=channel_details(\"channel_id\")\n",
    "         st.sucess(insert)   \n",
    "if st.button(\"Migrate to SQL\"):\n",
    "    Table=tables() \n",
    "    st.success(Table)\n",
    "\n",
    "show_table=st.radio(\"SELECT THE TABLE FOR VIEW\",(\"CHANNELS\",\"VIDEOS\",\"COMMENTS\"))     \n",
    "\n",
    "if show_table==\"CHANNELS\":\n",
    "   show_channels_table()\n",
    "\n",
    "elif show_table==\"VIDEOS\":\n",
    "    show_videos_table()\n",
    "\n",
    "elif show_table==\"COMMENTS\":\n",
    "    show_comments_table() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 22:23:53.221 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\nandh\\Desktop\\New folder (2)\\.venv\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "#SQL connection\n",
    "mydb = psycopg2.connect(host=\"localhost\",\n",
    "                                user=\"postgres\",\n",
    "                                password=\"Ramraji\",\n",
    "                                database=\"youtube_data\",\n",
    "                                port=\"5432\")\n",
    "cursor = mydb.cursor()\n",
    "\n",
    "question=st.selectbox(\"Select your Question\",(\"1.What are the names of all the videos and their corresponding channels?\",\n",
    "                                                \"2.Which channels have the most number of videos, and how many videos do they have?\",\n",
    "                                                \"3.What are the top 10 most viewed videos and their respective channels?\",\n",
    "                                                \"4.How many comments were made on each video, and what are their corresponding video names?\",\n",
    "                                                \"5.Which videos have the highest number of likes, and what are their corresponding channel names?\",\n",
    "                                                \"6.What is the total number of likes and dislikes for each video, and what are their corresponding video names?\",\n",
    "                                                \"7.What is the total number of views for each channel, and what are their corresponding channel names?\",\n",
    "                                                \"8.What are the names of all the channels that have published videos in the year 2022?\",\n",
    "                                                \"9.What is the average duration of all videos in each channel, and what are their corresponding channel names?\",\n",
    "                                                \"10.Which videos have the highest number of comments, and what are their corresponding channel names?\"\n",
    "                                                ))\n",
    "\n",
    "if question==\"1.What are the names of all the videos and their corresponding channels?\":\n",
    "    query1='''select title as videostitle,channel_name as channelname from videos'''\n",
    "    cursor.execute(query1)\n",
    "    mydb.commit()\n",
    "    t1=cursor.fetchall()\n",
    "    df1=pd.DataFrame(t1,columns=[\"videostitle\",\"channelname\"])\n",
    "    st.write(df1)\n",
    "\n",
    "elif question==\"2.Which channels have the most number of videos, and how many videos do they have?\":\n",
    "    query2='''select channel_name as channelname,total_videos as noofvideos from channels order by total_videos desc'''\n",
    "    cursor.execute(query2)\n",
    "    mydb.commit()\n",
    "    t2=cursor.fetchall()\n",
    "    df2=pd.DataFrame(t2,columns=[\"channelname\",\"noofvideos\"])\n",
    "    st.write(df2)\n",
    "elif question==\"3.What are the top 10 most viewed videos and their respective channels?\":\n",
    "    query3='''select views as views,channel_name as channelname,title as videotitle from videos where views is not null order by views desc limit 10'''\n",
    "    cursor.execute(query3)\n",
    "    mydb.commit()\n",
    "    t3=cursor.fetchall()\n",
    "    df3=pd.DataFrame(t3,columns=[\"views\",\"channelname\",\"videotitle\"])\n",
    "    st.write(df3) \n",
    "\n",
    "elif question==\"4.How many comments were made on each video, and what are their corresponding video names?\":\n",
    "    query4='''select comments as noofcomments,title as videotitle from videos where comments is not null'''\n",
    "    cursor.execute(query4)\n",
    "    mydb.commit()\n",
    "    t4=cursor.fetchall()\n",
    "    df4=pd.DataFrame(t4,columns=[\"noofcomments\",\"videotitle\"])\n",
    "    st.write(df4)\n",
    "\n",
    "elif question==\"5.Which videos have the highest number of likes, and what are their corresponding channel names?\":\n",
    "    query5='''select title as videotitle,channel_name as channelname,likes as likecount from videos where likes is not null order by likes desc'''\n",
    "    cursor.execute(query5)\n",
    "    mydb.commit()\n",
    "    t5=cursor.fetchall()\n",
    "    df5=pd.DataFrame(t5,columns=[\"videotitle\",\"channelname\",\"likecount\"])\n",
    "    st.write(df5)   \n",
    "\n",
    "elif question==\"6.What is the total number of likes and dislikes for each video, and what are their corresponding video names?\":\n",
    "    query6='''select likes as likecount,dislikes as dislikecount,title as videotitle from videos'''\n",
    "    cursor.execute(query6)\n",
    "    mydb.commit()\n",
    "    t6=cursor.fetchall()\n",
    "    df6=pd.DataFrame(t6,columns=[\"likecount\",\"dislikecount\",\"videotitle\"])\n",
    "    st.write(df6)       \n",
    "\n",
    "elif question==\"7.What is the total number of views for each channel, and what are their corresponding channel names?\":\n",
    "    query7='''select channel_name as channelname,views as totalviews,title as videotitle from channels'''\n",
    "    cursor.execute(query7)\n",
    "    mydb.commit()\n",
    "    t7=cursor.fetchall()\n",
    "    df7=pd.DataFrame(t7,columns=[\"channelname\",\"totalviews\",\"videotitle\"])\n",
    "    st.write(df7)    \n",
    "\n",
    "   \n",
    "elif question==\"8.What are the names of all the channels that have published videos in the year 2022?\":\n",
    "    query8='''select title as videotitle,published_date as videorelease,channel_name as channelname from videos where extract(year from published_date)=2022'''\n",
    "    cursor.execute(query8)\n",
    "    mydb.commit()\n",
    "    t8=cursor.fetchall()\n",
    "    df8=pd.DataFrame(t8,columns=[\"videotitle\",\"videorelease\",\"channelname\"])\n",
    "    st.write(df8)   \n",
    "\n",
    "elif question==\"9.What is the average duration of all videos in each channel, and what are their corresponding channel names?\":\n",
    "    query9='''select channel_name as channelname,AVG(duration) as averageduration from videos group by channel_name'''\n",
    "    cursor.execute(query9)\n",
    "    mydb.commit()\n",
    "    t9=cursor.fetchall()\n",
    "    df9=pd.DataFrame(t9,columns=[\"channelname\",\"averageduration\"])     \n",
    "\n",
    "    T9=[]\n",
    "    for index,row in df9.iterrow():\n",
    "        channel_title=row[\"channelname\"]\n",
    "        average_duration=row[\"averageduration\"]\n",
    "        average_duration_str=str(average_duration)\n",
    "        T9.append(dict(channeltitle=channel_title,avgduration=average_duration_str))\n",
    "    df=pd.DataFrame(T9)\n",
    "\n",
    "   \n",
    "elif question==\"10.Which videos have the highest number of comments, and what are their corresponding channel names?\":\n",
    "    query10='''select title as videotitle,channel_name as channelname,comments as comments from videos where comments is not order by comments desc'''\n",
    "    cursor.execute(query10)\n",
    "    mydb.commit()\n",
    "    t10=cursor.fetchall()\n",
    "    df10=pd.DataFrame(t10,columns=[\"videotitle\",\"channelname\",\"comments\"])\n",
    "    st.write(df10)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefinedColumn",
     "evalue": "column \"title\" does not exist\nLINE 1: ... channel_name as channelname,views as total_views,title as v...\n                                                             ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUndefinedColumn\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m cursor \u001b[38;5;241m=\u001b[39m mydb\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[0;32m      7\u001b[0m query7\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mselect channel_name as channelname,views as total_views,title as videotitle from channels\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery7\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m mydb\u001b[38;5;241m.\u001b[39mcommit()\n\u001b[0;32m     10\u001b[0m t7\u001b[38;5;241m=\u001b[39mcursor\u001b[38;5;241m.\u001b[39mfetchall()\n",
      "\u001b[1;31mUndefinedColumn\u001b[0m: column \"title\" does not exist\nLINE 1: ... channel_name as channelname,views as total_views,title as v...\n                                                             ^\n"
     ]
    }
   ],
   "source": [
    "mydb = psycopg2.connect(host=\"localhost\",\n",
    "                                user=\"postgres\",\n",
    "                                password=\"Ramraji\",\n",
    "                                database=\"youtube_data\",\n",
    "                                port=\"5432\")\n",
    "cursor = mydb.cursor()\n",
    "query7='''select channel_name as channelname,views as total_views,title as videotitle from channels'''\n",
    "cursor.execute(query7)\n",
    "mydb.commit()\n",
    "t7=cursor.fetchall()\n",
    "df7=pd.DataFrame(t7,columns=[\"channel name\",\"total_views\",\"video title\"])\n",
    "st.write(df7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'psycopg2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mydb \u001b[38;5;241m=\u001b[39m \u001b[43mpsycopg2\u001b[49m\u001b[38;5;241m.\u001b[39mconnect(host\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      2\u001b[0m                                 user\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostgres\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m                                 password\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRamraji\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m                                 database\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myoutube_data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m                                 port\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5432\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m cursor \u001b[38;5;241m=\u001b[39m mydb\u001b[38;5;241m.\u001b[39mcursor()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'psycopg2' is not defined"
     ]
    }
   ],
   "source": [
    "mydb = psycopg2.connect(host=\"localhost\",\n",
    "                                user=\"postgres\",\n",
    "                                password=\"Ramraji\",\n",
    "                                database=\"youtube_data\",\n",
    "                                port=\"5432\")\n",
    "cursor = mydb.cursor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
